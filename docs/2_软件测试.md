# 2. 软件测试文档

## 2.1 引言

本文档旨在概述 AI 推理服务器的软件测试策略和计划。全面的测试是确保软件质量、稳定性和可靠性的关键环节，尤其对于需要处理实时数据流和复杂 AI 计算的系统而言。

## 2.2 测试目标

*   验证核心功能的正确性，包括 RTSP 视频流接收、GStreamer Pipeline 构建与运行、视频帧提取、Roboflow Inference Pipeline 推理以及 API 接口的正常响应。
*   评估系统在不同负载下的性能和稳定性。
*   确保配置的灵活性和正确性（例如，不同 AI 模型、不同 GStreamer 参数）。
*   验证错误处理机制的有效性。
*   确保系统在目标部署环境（包括 Docker 容器）中能够正确运行。

## 2.3 测试范围

测试将覆盖以下主要方面：

*   **单元测试**: 针对项目中的独立模块和函数进行测试。
*   **集成测试**: 测试模块间的交互和数据流转。
*   **端到端测试 (E2E)**: 模拟真实用户场景，从客户端推流到服务器返回推理结果的完整流程。
*   **性能测试**: 评估系统在高并发连接、高码率视频流等压力下的表现。
*   **兼容性测试**: 测试与不同 Android 客户端或 RTSP 推流工具的兼容性。

## 2.4 测试策略与方法

### 2.4.1 单元测试

*   **工具**: Pytest, `unittest.mock` (用于模拟依赖).
*   **测试文件位置**: `src/tests/` 目录。
*   **覆盖的核心模块及策略**:
    *   **`src/app/services/websocket_manager.py` (测试文件: `src/tests/test_websocket_manager.py`)**:
        *   测试 `ConnectionManager` 类的核心功能。
        *   覆盖客户端连接 (`connect`)、断开连接 (`disconnect`)。
        *   验证个人消息发送 (`send_personal_message`) 和广播 (`broadcast`, `broadcast_ai_result`) 的逻辑，包括正确处理JSON和文本消息。
        *   测试在发送消息出错时自动断开客户端的机制。
        *   验证 `ping` 任务的启动和取消逻辑。
        *   使用 `unittest.mock.AsyncMock` 模拟 `fastapi.WebSocket` 对象。

    *   **`src/app/services/gstreamer_frame_producer.py` (测试文件: `src/tests/test_gstreamer_frame_producer.py`)**:
        *   测试 `GStreamerFrameProducer` 类的功能，该类作为 GStreamer appsink 和 Roboflow InferencePipeline 之间的桥梁。
        *   验证初始化、启动 (`start`) 和基本属性获取 (`get_fps`, `get_resolution`, `isOpened`)。
        *   核心测试 `read_frame` 方法：
            *   成功从模拟队列读取帧数据和时间戳，并正确转换为 `VideoFrame` 对象。
            *   处理队列为空、生产者未运行的情况。
            *   测试时间戳转换错误（如无效时间戳、None时间戳）的回退逻辑，并使用 `caplog` 检查日志输出。
        *   测试 `release` 方法是否能正确清理帧队列。
        *   测试 `grab` 和 `retrieve` 方法的帧抓取和检索逻辑，以及内部状态（如 `_last_read_video_frame`）的更新。
        *   验证 `discover_source_properties` 方法返回正确的源属性。
        *   使用 `queue.Queue` 模拟帧数据源，使用 `numpy` 创建模拟帧数据。

    *   **`src/app/services/ai_processor.py` (测试文件: `src/tests/test_ai_processor.py`)**:
        *   测试 `AIProcessor` 类的复杂逻辑，包括与 Roboflow SDK 和事件循环的交互。
        *   模拟核心依赖项：
            *   `app.services.ai_processor.get_model` (避免实际模型加载)。
            *   `app.services.ai_processor.InferencePipeline` (模拟 Roboflow 推理管道)。
            *   `app.services.ai_processor.GStreamerVideoSource` (模拟视频源)。
            *   `GStreamerFrameProducer` (作为输入给 `AIProcessor`)。
        *   测试内部工具方法：
            *   `_predictions_to_dict`: 验证不同类型的输入（None, Roboflow SDK 对象, 字典, `supervision.Detections` 对象）能否正确转换为标准字典格式。对 `supervision.Detections` 的测试会根据其可用性跳过。
            *   `_extract_frame_details`: 验证能否从模拟的视频帧对象或NumPy数组中正确提取帧ID、时间戳和图像形状。
        *   测试核心流程：
            *   `start()`: 验证在有/无 `frame_producer` 情况下的行为，检查依赖项的模拟方法是否被正确调用 (如 `mock_frame_producer.start`, `MockVideoSource.start`, `MockInferencePipeline.start`)，以及内部状态（如 `is_running`, `main_event_loop`）的设置。
            *   `stop()`: 验证能否正确终止推理管道并释放资源，检查 `inference_pipeline.terminate` 和 `frame_producer.release/stop` 是否被调用。
            *   `_on_prediction()`: 验证当收到模拟的预测结果和视频帧时，能否正确调用 `_extract_frame_details`，并且（如果提供了 `on_prediction_callback`）能否使用 `asyncio.run_coroutine_threadsafe` 将回调函数正确调度到事件循环。使用 `AsyncMock` 测试回调的调用。
        *   使用 `@patch` 装饰器进行细粒度的模拟，使用 `monkeypatch` 设置模拟配置。
        *   涉及 `asyncio` 的测试使用 `pytest.mark.asyncio`，并妥善管理事件循环的创建和清理。

    *   **`src/rtspserver.py` (测试文件: `src/tests/test_rtspserver.py`)**:
        *   主要测试 `PushedStreamFactory` 类的 `do_create_element` 方法。
        *   由于 GStreamer 依赖的特殊性，测试文件会尝试导入 GStreamer 库 (`Gst`, `GstRtspServer`, `GLib`)；如果导入失败，相关测试将被跳过 (`pytest.skip`)。
        *   使用 `@patch('rtspserver.Gst.parse_launch')` 模拟 `Gst.parse_launch` 调用。
        *   验证 `do_create_element` 在成功解析 pipeline 字符串时，是否使用预期的参数调用 `Gst.parse_launch` 并返回模拟的 pipeline 对象。
        *   验证在 `Gst.parse_launch` 抛出 `GLib.Error` (模拟解析失败) 时，`do_create_element` 是否返回 `None` 并记录错误日志 (通过 `caplog` 检查)。
        *   模拟 `GstRtspServer.RTSPUrl` 对象以提供给 `do_create_element` 方法。
        *   单元测试集中于工厂类的逻辑，`main()` 函数因涉及 GLib 主循环和实际服务器启动，更适合集成测试。

### 2.4.2 集成测试

*   **目标**: 验证 GStreamer 与 FastAPI 应用的集成，以及 FastAPI 应用内部服务模块与 API 模块的集成。
*   **场景**:
    *   **GStreamer 与视频帧处理**: 测试能否正确从 GStreamer 管道获取视频帧并传递给后续处理模块。
    *   **视频帧处理与 Roboflow 推理**: 测试视频帧能否正确传递给 Roboflow Inference Pipeline，并获得预期的推理结果（可能需要使用 Roboflow 提供的测试模式或模拟推理结果）。
    *   **API 与服务层**: 测试通过 API 调用能否正确触发服务层逻辑并返回结果。
*   **工具**: Pytest 结合 FastAPI 的 `TestClient`。

### 2.4.3 端到端测试 (E2E)

*   **目标**: 模拟完整的用户操作流程。
*   **步骤**:
    1.  启动 AI 推理服务器 (本地或 Docker 环境)。
    2.  使用一个模拟的 RTSP 推流客户端 (可以是 GStreamer 命令行工具 `gst-launch-1.0`, FFmpeg, 或一个简单的 Python/Java 脚本) 向服务器推流一个标准测试视频文件 (存放于 `src/videos/` 目录可能是一个好主意)。
    3.  通过 API (或 WebSocket 客户端，如项目中的 `test_websocket_client.py`) 检查推理结果是否符合预期（例如，检测到视频中的特定对象）。
    4.  测试不同配置下的表现，如更换不同的 Roboflow 模型。
*   **关注点**: 连接稳定性、数据处理的延迟、结果的准确性。

### 2.4.4 性能测试

*   **目标**: 评估系统在压力下的表现，找出性能瓶颈。
*   **指标**:
    *   最大并发 RTSP 连接数。
    *   不同分辨率和码率视频流的处理能力 (FPS)。
    *   推理延迟 (从接收帧到输出结果的时间)。
    *   CPU 和内存使用率。
*   **工具**:
    *   RTSP 压力测试工具 (如 `rtsp-benchmark` 或自定义脚本)。
    *   Python性能分析工具 (如 `cProfile`, `py-spy`)。
    *   系统监控工具 (如 `htop`, `vmstat`, Prometheus)。
*   **场景**:
    *   逐步增加 RTSP 客户端连接数。
    *   使用不同清晰度、码率的视频流进行测试。
    *   长时间运行稳定性测试 (例如，连续运行24小时)。

### 2.4.5 兼容性测试

*   **目标**: 确保服务器能与主流的 RTSP 推流源 (尤其是目标 Android 应用所使用的库) 良好兼容。
*   **方法**: 使用不同的 RTSP 客户端软件或设备进行推流测试。

### 2.4.6 实机场地测试 (Field Testing)

#### 2.4.6.1 测试目标

*   验证端到端系统在真实室外环境下的盲道（或其他选定目标物体）识别功能的稳定性和准确性。
*   评估在不同光照条件、天气状况（如晴天、阴天）、路面状况和用户拍摄角度/移动状态下的模型表现。
*   检验移动端App与服务器之间数据传输的流畅性和感知延迟。
*   收集真实场景数据，用于潜在的模型优化、问题分析和演示。

#### 2.4.6.2 测试环境与准备

*   **硬件**:
    *   Android 智能手机：安装有配置好的RTSP推流应用程序。
    *   AI 推理服务器：按照部署指南在目标硬件（如开发板、PC或云服务器）上配置并运行。
    *   网络连接：确保手机和服务器之间有稳定、足够的带宽（例如，通过同一局域网的Wi-Fi，或手机使用移动数据，服务器有公网IP/端口映射）。
*   **软件**:
    *   服务器端：AI 推理服务（包括GStreamer RTSP接收服务和FastAPI应用）已按`3_发布和运行.md`文档正确启动并运行。
    *   客户端：Android App 中的RTSP推流参数（服务器IP、端口、路径等）已正确配置。
*   **场地**:
    *   选择包含清晰盲道（或其他测试目标）的典型室外场景，如人行道、广场等。
    *   尽量覆盖不同光照条件：例如，上午/下午的阳光直射区域、建筑物阴影区域。
    *   （可选）包含一些潜在干扰因素的场景，如地面其他标记、部分遮挡物等。
*   **辅助工具 (可选)**:
    *   另一台手机或相机：用于记录测试过程的视频或照片，方便后续分析和结果比对。
    *   秒表或录屏计时：用于粗略估算端到端延迟。
    *   笔记工具：记录观察到的现象和即时结果。

#### 2.4.6.3 测试步骤

1.  **系统启动与连接检查**:
    *   启动AI推理服务器，并确认所有服务（RTSP接收、FastAPI、AI模型加载）均正常运行，无错误日志。
    *   在Android手机上打开RTSP推流App。
    *   配置App将视频流推向服务器的RTSP地址 (例如 `rtsp://<服务器IP>:8554/push` 或其他自定义路径)。
    *   开始推流。
    *   在服务器端通过日志、监控工具或API响应确认视频流已成功接收并正在被处理。
2.  **场景遍历与数据采集**:
    *   **场景 A: 理想条件下的盲道识别**
        *   测试员手持手机，以正常步行速度，摄像头以自然角度（模拟用户视角）对准清晰、光照良好的直线型盲道。
        *   观察并记录服务器返回的识别结果（如通过WebSocket实时显示或后续查询API）。
    *   **场景 B: 不同光照条件下的盲道识别**
        *   在光线较暗的区域（如树荫下、傍晚）或光线过曝区域重复场景A的测试。
    *   **场景 C: 盲道部分遮挡/干扰**
        *   模拟盲道上有少量行人脚、落叶、小摊贩物品等轻微遮挡的情况。
        *   测试是否存在其他颜色、形状相似的地面标记被误识别为盲道。
    *   **场景 D: 用户行为模拟**
        *   测试员在拍摄盲道时，进行一些常见的用户行为，如：手机晃动、行走中途停顿、转身、摄像头角度轻微变化（俯仰、左右）。
    *   **场景 E: 连续运行与稳定性**
        *   让系统连续运行一段时间（例如10-15分钟），持续推流并观察识别结果的稳定性，有无明显性能下降或错误累积。
3.  **结果记录**:
    *   对每个测试场景，详细记录手机App端的推流情况（是否流畅、有无卡顿或断开）。
    *   记录服务器端返回的推理结果：盲道是否被检测到、检测框的位置和大小、置信度分数。
    *   记录任何明显的错误识别（将非盲道识别为盲道）或漏识别（未能识别出存在的盲道）。
    *   估算并记录从手机拍摄到接收到可用的推理结果之间的主观感知延迟。
    *   记录测试时的天气、具体时间和地点。

#### 2.4.6.4 关键观察指标

*   **盲道识别准确率**: 系统能否在有效范围内准确、稳定地识别出盲道区域。
*   **误报率 (False Positive Rate)**: 将非盲道物体错误识别为盲道的频率。
*   **漏报率 (False Negative Rate)**: 未能识别出实际存在的盲道的频率。
*   **识别稳定性/连续性**: 识别结果（如边界框）是否会剧烈跳动，或者在目标持续可见时是否能连续输出。
*   **端到端延迟**: 用户从手机摄像头观察到场景，到手机App（或配套设备）给出基于AI服务器分析结果的反馈所需的时间。
*   **系统鲁棒性**: 在不同光照、移动、轻微遮挡等干扰下，系统维持其性能水平的能力。
*   **用户体验**: 推流App的流畅性，服务器响应的及时性，整体操作的便捷性。

#### 2.4.6.5 测试结果记录表 (示例与占位符)

*(请在实际测试后，根据观察到的情况填充此表或创建类似的详细记录)*

| 测试日期与时间   | 测试地点        | 天气状况 | 测试场景描述                                      | 手机App推流情况 | 服务器识别结果 (盲道)                                     | 感知延迟 (估算)  | 效果评估/问题记录                                          |
| ---------------- | --------------- | -------- | ------------------------------------------------- | --------------- | --------------------------------------------------------- | ---------------- | ---------------------------------------------------------- |
| YYYY-MM-DD HH:MM | 校园A区域人行道 | 晴朗     | **场景A.1**: 直线盲道，光照良好，匀速前进         | 流畅            | 持续准确识别，置信度平均0.9，边界框稳定                   | ~1.0-1.5秒       | 效果符合预期。                                             |
| YYYY-MM-DD HH:MM | 校园A区域人行道 | 晴朗     | **场景A.2**: 转弯盲道，光照良好，匀速前进         | 流畅            | 转弯处识别略有延迟，但能跟上，置信度0.8-0.85              | ~1.2-1.8秒       | 转弯处理基本可以，偶有框不完全贴合。                       |
| YYYY-MM-DD HH:MM | 教学楼北侧路    | 阴影中   | **场景B.1**: 直线盲道，大部分在建筑物阴影下       | 流畅            | 识别率下降，置信度降至0.6-0.7，部分帧漏检，边界框抖动增加 | ~1.5-2.5秒       | 暗光下性能衰退明显，需优化模型或图像增强。                 |
| YYYY-MM-DD HH:MM | 商业街边        | 多云     | **场景C.1**: 盲道上有少量散落的传单和行人脚踝遮挡 | 流畅            | 未遮挡部分能识别，但被遮挡区域易漏检，偶尔框选整个大区域  | ~1.5-2.0秒       | 对小面积遮挡鲁棒性一般。                                   |
| YYYY-MM-DD HH:MM | 校园B区域广场   | 晴朗     | **场景C.2**: 旁边有黄色消防栓（颜色相似）         | 流畅            | 未误识别消防栓为盲道                                      | ~1.0-1.5秒       | 对此特定颜色干扰物表现良好。                               |
| YYYY-MM-DD HH:MM | 校园A区域人行道 | 晴朗     | **场景D.1**: 快速跑动并拍摄盲道                   | 画面有拖影      | 识别几乎完全失败，或产生大量错误/跳动框                   | >3秒或无有效结果 | 快速运动时无法有效处理，可能超出当前帧率或模型处理能力。   |
| YYYY-MM-DD HH:MM | 任何地点        | -        | **场景E.1**: 连续推流15分钟，手机静置对准某一场景 | 持续流畅        | 初期识别稳定，10分钟后偶有卡顿后恢复的现象（待查服务器）  | ~1.0-1.5秒       | 长时间运行基本稳定，需关注服务器资源是否有缓慢泄露或过载。 |
| ...              | ...             | ...      | ...                                               | ...             | ...                                                       | ...              | ...                                                        |

*附注: 上表中的"服务器识别结果"和"感知延迟"等均为示例，实际测试时应详细记录具体数值和现象。置信度分数、边界框的具体表现（如是否精确、有无漂移）也应是关注重点。*

## 2.5 测试环境

*   **开发环境**: 开发人员本地机器，用于单元测试和初步集成测试。
*   **测试环境**: 与生产环境相似的独立环境，最好是 Docker 容器化环境，用于完整的集成测试、E2E 测试和性能测试。使用 `Dockerfile` 和 `docker-compose.yml` 确保环境一致性。

## 2.6 测试用例管理 (建议)

对于比赛项目，可以简化管理，但至少应有核心功能的测试用例列表。

| 测试用例 ID | 模块      | 测试类型 | 描述                           | 预期结果                                | 实际结果 | 状态 |
| ----------- | --------- | -------- | ------------------------------ | --------------------------------------- | -------- | ---- |
| TC_RTSP_001 | RTSP 服务 | E2E      | 客户端成功连接并推流           | 服务器接收到视频流，无错误日志          |          |      |
| TC_INF_001  | 推理服务  | 集成     | 输入测试图片，调用推理接口     | 返回正确的推理结果 (e.g., 边界框和类别) |          |      |
| TC_API_001  | API       | 集成     | 调用 `/status` (假设存在) 接口 | 返回 `{"status": "ok"}`                 |          |      |
| ...         | ...       | ...      | ...                            | ...                                     |          |      |

## 2.7 缺陷管理 (建议)

使用简单的缺陷跟踪方式，例如共享文档或项目管理工具的 issues 功能，记录发现的问题、复现步骤、优先级和修复状态。

## 2.8 自动化测试

*   单元测试已通过 Pytest 实现自动化，可方便地在开发和 CI 流程中运行。
*   `build.sh` 或 `fix-docker-network.sh` 脚本可以用于构建和准备测试环境。

## 2.9 总结

通过上述测试策略和方法的实施，可以有效地保证 AI 推理服务器的质量和可靠性，为比赛的顺利进行提供坚实的基础。单元测试的补充增强了代码的健壮性，而端到端测试和集成测试对于验证系统整体功能至关重要。 